"""Helpful functions for the second BE537 Python assignment."""

# Import required libraries
import os
import numpy as np
import nibabel as nib
import torch
import torch.nn.functional as tfun
from scipy import interpolate, signal, cluster
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import matplotlib.colors as colors
from be537hw1 import *


def my_read_pytorch_warp_from_nifti(filename, **kwargs):
    """
    Read an ANTs/Greedy displacement field from a NIfTI file and return as a PyTorch tensor.
    
    :param filename:    Path to a NIfTI file storing the displacement
    
    :Keyword Arguments: You can supply parameters 'dtype' and 'device' that will be used to
                        initialize the PyTorch tensor
                        
    :returns:           Shape [1,D,H,W,3] PyTorch tensor holding the displacement field. The
                        displacement field will be in the PyTorch normalized coordinates, i.e.,
                        see torch.nn.functional.grid_sample
    """
    
    # Read the data
    img = nib.load(filename)
    
    # Extract the data, convert to PyTorch, and permute axes to match affine_grid
    T = torch.tensor(img.get_fdata(), **kwargs).permute(3,0,1,2,4)
    
    # Displacements in Greedy are in physical units. They need to be converted to voxel units
    # and then to the PyTorch normalized coordinate system. Mapping to voxel units is given
    # by the inverse of the NIfTI s-form matrix. Mapping to PyTorch units is from the first 
    # homework asssignment. Since we are transforming displacement vectors and not points, we
    # do not need to worry about the translation component of the affine transform
    W, _ = my_pytorch_coord_transform(img.shape[:3])
    S_inv = np.linalg.inv(img.affine[:3,:3])
    M_LPS = np.diag(np.array([-1,-1,1]))
    A = W @ S_inv @ M_LPS
    
    # Apply the transformation A to the displacements
    T_pt = my_apply_affine_to_pytorch_grid(A, torch.zeros(3), T) 
    
    return T_pt, img.header


def my_read_pytorch_image_from_nifti(filename, **kwargs):
    """
    Read a 3D or 4D scalar image from a NIfTI file, return as a PyTorch tensor
    
    :param filename:    Path to a NIfTI file storing the image
    
    :Keyword Arguments: You can supply parameters 'dtype' and 'device' that will be used to
                        initialize the PyTorch tensor
                        
    :returns:           Shape [1,C,D,H,W] PyTorch tensor holding the image
    """
    img = nib.load(filename)
    T = torch.tensor(img.get_fdata(), **kwargs)
    while len(T.shape) < 5:
        T = T.unsqueeze(0)  
    return T, img.header


def my_view_pytorch_image(img, header, **kwargs):
    """Wrapper that calls my_view for PyTorch tensors"""
    my_view(img.detach().squeeze().cpu().numpy(), header, **kwargs)
    
    
def my_gaussian_fft_kernel(I_ref, sigma):
    """
    Generate the FFT of a Gaussian kernel that can be used for FFT-based Gaussian smoothing 
    
    :param I_ref: 5D tensor that will be convolved with the kernel, used to infer the 
                  shape, data type and device for the output kernel
    :param sigma: standard deviation of the Gaussian kernel
    :param dtype: data type of the output tensor (default: torch.float32)
    :returns: 5D tensor that can be multiplied by the image RFFT for FFT-based smoothing
    """
    
    # Generate a Gaussian kernel, convert to PyTorch
    K = torch.tensor(my_gaussian_3d(sigma), 
                     dtype=I_ref.dtype, device=I_ref.device).unsqueeze(0).unsqueeze(0)
    
    # Normalize the kernel
    K = K / K.sum()
    
    # Pad the kernel so it is centered in an image of size I_ref.shape
    ab_pad = np.array(I_ref.shape[2:5]) - np.array(K.shape[2:5])
    a_pad = np.floor(ab_pad / 2)
    b_pad = ab_pad - a_pad
    pad_5d = tuple(np.flip(np.vstack((a_pad, b_pad)).T.flatten()).astype(int))+(0,0,0,0)
    K_pad = tfun.pad(K,pad_5d)
    
    # Return the real FFT of the kernel
    return torch.fft.rfftn(K_pad)
    
    
def my_pytorch_gaussian_lpf(img, sigma=None, kernel=None):
    """
    Apply Gaussian smoothing to a 3D image represented as a PyTorch tensor
    
    You can pass in the sigma of the Gaussian or a 5D tensor representing the Gaussian
    kernel (generated using `my_gaussian_fft_kernel`). The latter is faster if you will
    be making repeated calls to this function.
    
    :param img: Input image, represented as a 5D tensor
    :param sigma: Standard deviation of the Gaussian kernel. If `kernel` parameter
        is not supplied, the kernel will be generated by calling `my_gaussian_fft_kernel`
    :param kernel: Gaussian kernel generated by `my_gaussian_fft_kernel`. 
    """
    
    # Either sigma or kernel must be provided
    assert sigma is not None or kernel is not None
    if kernel is None:
        kernel = my_gaussian_fft_kernel(img, sigma)
    
    # Create a Gaussian kernel
    img_fft = torch.fft.rfftn(img)
    return torch.fft.fftshift(torch.fft.irfftn(img_fft * kernel))


def my_pytorch_gaussian_lpf_transform(v, sigma=None, kernel=None):
    """
    Apply Gaussian smoothing to a transform represented as a PyTorch tensor
    
    You can pass in the sigma of the Gaussian or a 5D tensor representing the Gaussian
    kernel (generated using `my_gaussian_fft_kernel`). The latter is faster if you will
    be making repeated calls to this function.
    
    :param img: Input spatial transformation, represented as a [N,D,H,W,3] tensor
    :param sigma: Standard deviation of the Gaussian kernel. If `kernel` parameter
        is not supplied, the kernel will be generated by calling `my_gaussian_fft_kernel`
    :param kernel: Gaussian kernel generated by `my_gaussian_fft_kernel`. 
    """
    
    # Make sure the image is formatted as a transform
    assert v.shape[4]==3

    # Convert the transform into a multi-channel image of shape [1,3,D,H,W]
    v_chan = v.permute(0,4,1,2,3)
    
    # Either sigma or kernel must be provided
    assert sigma is not None or kernel is not None
    if kernel is None:
        kernel = my_gaussian_fft_kernel(v_chan, sigma)
    
    # Perform FFT convolution but only along image dimensions
    v_fft = torch.fft.rfftn(v_chan, dim=(-3,-2,-1))
    v_sm_chan = torch.fft.fftshift(
        torch.fft.irfftn(v_fft * kernel, dim=(-3,-2,-1)),
        dim=(-3,-2,-1))
    
    # Reorder dimensions again
    return v_sm_chan.permute(0,2,3,4,1)


def my_image_downsample(img, factor):
    """
    Downsample a 3D image with antialiasing
    
    :param img: Input image represented as a PyTorch [1,C,D,H,W] tensor
    :param factor: Downsampling factor
    :returns: Output image represented as a PyTorch [1,C,D,H,W] tensor
    """
    
    # Apply smoothing to the image
    img_sm = my_pytorch_gaussian_lpf(img, 0.5 * factor)
    
    # Determine the output size
    sz_in  = np.array(img_sm.shape[2:5])
    sz_out = np.int32(np.ceil(np.array(img_sm.shape[2:5]) / factor))

    # Perform interpolation
    return tfun.interpolate(img_sm, tuple(sz_out))


def my_adjust_nifti_header_for_resample(header, new_shape):
    """
    Adjust NIFTI header for change in image dimensions. 
    
    We assume that the image still occupies the same physical space but that the voxel dimensions (shape)
    has changed. We update the zooms and the sform in the new image
    """
    
    # Read header information
    sz_in = np.array(header.get_data_shape())
    sp_in = np.array(header.get_zooms())
    sf_in = np.array(header.get_sform())
    
    # Prepare the new information
    sz_out = np.array(new_shape)
    sp_out = sp_in * sz_in / sz_out
    sf_out = np.eye(4)
    
    # Update the sform elements
    sf_out[0:3,0:3] = sf_in[0:3,0:3] * np.diag(sp_out / sp_in)
    sf_out[0:3,3] = sf_in[0:3,3] - 0.5 * (sp_in - sp_out)
    
    # Update the dimensions of the image
    header_new = header.copy()
    header_new.set_data_shape(sz_out)
    header_new.set_zooms(sp_out)
    header_new.set_sform(sf_out)
        
    return header_new


def my_contour_values(data, k):
    """Automatically guess contour values for an image using k-means clustering"""
    flat = data.flatten()
    q_range = np.quantile(flat, [0, .98])
    kmeans_guess = np.linspace(q_range[0],q_range[1],k+1)
    kmeans_val,_ = cluster.vq.kmeans(flat, kmeans_guess)
    return np.convolve(kmeans_val, np.array([0.5,0.5]),mode='valid')


def my_view_overlay(img, ovl, header=None, xhair=None, crange=None, cmap='gray', levels=4, alpha=0.7):
    """Visualize alignment between two images as a contour overlay
    
    :param img: main image (3D voxel array or PyTorch tensor)
    :param ovl: overlay image (3D voxel arrayor PyTorch tensor)
    :param header: Image header (returned by my_read_nifti)
    :param xhair: Crosshair position (1D array or tuple)
    :param crange: Intensity range, a tuple with minimum and maximum values
    :param cmap: Colormap (a string, see matplotlib documentation)
    :param levels: Number of contours to plot for the overlay (default: 4)
    :param alpha: Overlay opacity (default: 0.7)
    """
    # Convert from tensors to numpy
    if isinstance(img, torch.Tensor):
        img = img.squeeze().detach().cpu().numpy()
        
    if isinstance(ovl, torch.Tensor):
        ovl = ovl.squeeze().detach().cpu().numpy()
    
    fig, axs = plt.subplots(2,2)
    xhair = np.array(img.shape)//2 if xhair is None else xhair
    crange = (np.min(img),np.quantile(img,0.99)) if crange is None else crange
    sp = header.get_zooms() if header is not None else np.ones((len(img.shape),1))
    
    # Generate contour values automatically using k-means
    level_vals = my_contour_values(ovl, levels)
    
    im0 = my_view_axis(axs[0,0], img[:,:,xhair[2]].T, 
                       aspect=sp[1]/sp[0], xhair=(xhair[0],xhair[1]), cmap=cmap, crange=crange)
    axs[0,0].contour(ovl[:,:,xhair[2]].T, levels=level_vals, alpha=alpha)
    im1 = my_view_axis(axs[0,1], img[xhair[0],:,:].T, 
                       aspect=sp[2]/sp[1], xhair=(xhair[1],xhair[2]), cmap=cmap, crange=crange)
    axs[0,1].contour(ovl[xhair[0],:,:].T, levels=level_vals, alpha=alpha)
    im2 = my_view_axis(axs[1,1], img[:,xhair[1],:].T, 
                       aspect=sp[2]/sp[0], xhair=(xhair[0],xhair[2]), cmap=cmap, crange=crange)
    axs[1,1].contour(ovl[:,xhair[1],:].T, levels=level_vals, alpha=alpha)
    axs[1,0].axis('off');
    cax = plt.axes([0.175, 0.15, 0.3, 0.05])
    plt.colorbar(im1, orientation='horizontal', ax=axs[1,1], cax=cax)

    
def my_view_multi(img, header=None, xhair=None, 
                  crange=None, cmap='gray', 
                  is_warp=False, stride=None, transpose=False,
                  figwidth=None, layout='AS;LC', 
                  title=None, axis_visible=True):
    """
    Display multiple 3D images side by side in ITK-SNAP like layout
    
    This function can be used to visualize multiple 3D images side by side in axial,
    sagittal and coronal views. The input can be a 5D PyTorch tensor or a Python tuple
    or list of 3D PyTorch tensors or Numpy arrays. 
    
    Args:
    
        img (torch.Tensor): A 5D tensor storing one or more 3D images.
            The tensor should have the shape [N C D H W] unless `is_warp=True`
            in which case it should have the shape [N D H W 3]. The plots will
            be arranged in N rows and C columns (unless `transpose=True`)
            
        header (optional): Nibabel Nifti image header (returned by `my_read_nifti`)
        
        xhair (optional): Crosshair coordinates, iterable of size 3
        
        crange (optional): Intensity range, iterable of size 2 with min and max 
            values. If not specified, range will be determined from image quantiles
            
        cmap (str): Colormap string (see matplotlib documentation). Default is `gray`
        
        is_warp (Boolean, optional): If True, treat the 5D tensor as a spatial 
            transformation, i.e., having the shape [N D H W C]. Default is False.
            
        stride (int, optional): Override default arrangement of the plot into rows and 
            columns. The number of rows will be equal to `stride`.
            
        transpose (Boolean, optional): Transpose rows/columns. 
            If True, minibatches will be plotted along rows rather than columns 
            (default is False)
            
        figwidth (int, optional): Width of the figure in inches

        title (str, optional) Main figure title
        
        layout (str, optional) Specify how to lay out the axial, coronal and sagittal 
            panels in the plot. The string should consist of characters 'ASCL' with 
            semicolons separating rows. 'L' stands for 'legend'. The standard layout
            is 'AS;LC'. If you only want to show coronal slices, use 'C'.
            
        axis_visible (Boolean, optional) Set to False to hide the axes around the plots
                
    """
    # The input can be a 5D tensor, in which case we lay things out by minibatch/channel
    if isinstance(img, torch.Tensor) and len(img.shape) == 5:
        
        # Reorganize the tensor into a list of 3D images
        img_chan = img if is_warp is False else img.permute(0,4,1,2,3)
        img_chan = img_chan if transpose is False else img_chan.permute(1,0,2,3,4)
        img_list = img_chan.flatten(0, 1).split(1)
        stride = img_chan.shape[1] if stride is None else stride
        
        # Set the reference image for intensity range, etc
        img_ref = img_chan
        
        # Get the 3D shape
        dim = img_ref.shape[-3:]
        
    else:
        img_list = img
        stride = 1 if stride is None else stride
        img_ref = img_list[0]
        dim = img_list[0].shape[-3:]    
            
    # Generate default values for missing parameters
    xhair = np.array(dim) // 2 if xhair is None else xhair
    sp = header.get_zooms() if header is not None else np.ones(3)
        
    # Get the image range if needed
    if crange is None:
        if is_warp:
            q = torch.max(torch.abs(img_ref)).item()
            crange = (-q, q)
        else:
            q_pts = torch.tensor(np.array([0.01,0.99]), dtype=img_ref.dtype, device=img_ref.device)
            q_val = torch.quantile(img_ref, q_pts)
            crange = [q_val[0].item(), q_val[1].item()]    

    # Parse the layout parameter into an array
    lo = np.array([[w for w in x] for x in layout.split(';')])
    
    # Create grid specs for the 2D layouts
    fig = plt.figure()
    gs0 = gridspec.GridSpec(lo.shape[0], lo.shape[1], figure=fig)
    
    # Calculate total width and height of figure
    w_tot = (sp[0] * dim[0] + sp[1] * dim[1]) * stride
    h_tot = sp[2] * img_list[0].shape[2] * 2 * int(np.ceil(len(img_list) / stride))
        
    figasp = h_tot * 1.0 / w_tot

    # Spacings for panels
    panel_aspect = { 'A': sp[1]/sp[0], 'S': sp[2]/sp[1], 'C': sp[2]/sp[0] }
    panel_axis = { 'A': -1, 'S': -3, 'C': -2 }
    panel_xhair = { 'A': (xhair[0],xhair[1]), 'S': (xhair[1],xhair[2]), 'C': (xhair[0],xhair[2]) }
    
    # Layout within each panel
    pr,pc = stride,int(np.ceil(len(img_list) / stride))
    
    # Calculate total width and height in pixel units
    w_lo, h_lo = np.zeros(lo.shape), np.zeros(lo.shape)
    
    # Image list needs to be transposed for plotting
    idx_ax = np.array(range(len(img_list))).reshape(stride,-1).T.flatten().tolist()
    
    # Iterate over the grid specs
    for r,lo_r in enumerate(lo):
        for c,key in enumerate(lo_r):
            
            gs1 = gs0[r,c].subgridspec(pr, pc)
            
            if key == 'L':
                ax.axis('off')
                ax = plt.axes([0.1, 0.25, 0.3, 0.05 / figasp])
            
                norm = colors.Normalize(vmin=crange[0], vmax=crange[1])
                sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
                sm.set_array([])
                plt.colorbar(sm, orientation='horizontal', cax=ax)
                
            else:
                for i, img_i in enumerate(img_list):
                    # Extract a slice
                    slc = torch.narrow(img_i, panel_axis[key], xhair[panel_axis[key]], 1).squeeze()

                    # Plot the slice
                    ax = fig.add_subplot(gs1[idx_ax[i]])
                    ax.imshow(slc.transpose(-2,-1).squeeze().detach().cpu().numpy(), 
                              cmap=cmap, aspect=panel_aspect[key], 
                              vmin=crange[0], vmax=crange[1])
                    ax.invert_xaxis()
                    ax.invert_yaxis()

                    # Plot the crosshair
                    ax.axvline(x=panel_xhair[key][0], color='lightblue')
                    ax.axhline(y=panel_xhair[key][1], color='lightblue')
                    
                    ax.xaxis.set_visible(axis_visible)
                    ax.yaxis.set_visible(axis_visible)
                    
                # Calculate the widths and heights
                h_lo[r,c] = slc.shape[-2] * panel_aspect[key] * pr
                w_lo[r,c] = slc.shape[-1] * pc

    h_tot = np.sum(np.max(h_lo, axis=1))
    w_tot = np.sum(np.max(w_lo, axis=0))
                    
    # Decorate figure
    if title:
        fig.suptitle(title)
        
    if figwidth is not None:
        fig.set_figwidth(figwidth)  
        
    fig.set_figheight(fig.get_figwidth() * (h_tot * 1.0 / w_tot) * 1.2)


def my_transform_affine_and_warp(I_mov, grid, A=None, b=None, w=None, **kwargs):
    """
    Apply a combination of affine and deformable transforms to an image.
    
    This function transforms a moving image using a composition of an optional affine 
    transformation (A,b) and an optional displacement field (w) and returns a new image
    I_res such that
    
        I_res(x) = I_mov(A(x + w(x)) + b)
    
    If A, b or w are not supplied, they are assumed to be identity
    """
    px = grid[0,:,:,:,0]
    py = grid[0,:,:,:,1]
    pz = grid[0,:,:,:,2]
    
    wx = w[0,:,:,:,0] if w is not None else torch.zeros_like(px)
    wy = w[0,:,:,:,1] if w is not None else torch.zeros_like(py)
    wz = w[0,:,:,:,2] if w is not None else torch.zeros_like(pz)
    
    if A is None:
        A = torch.eye(3, dtype=grid.dtype, device=grid.device)
    
    if b is None:
        b = torch.zeros(3, dtype=grid.dtype, device=grid.device)
    
    qx = A[0,0]*(px+wx) + A[0,1]*(py+wy) + A[0,2]*(pz+wz) + b[0]
    qy = A[1,0]*(px+wx) + A[1,1]*(py+wy) + A[1,2]*(pz+wz) + b[1]
    qz = A[2,0]*(px+wx) + A[2,1]*(py+wy) + A[2,2]*(pz+wz) + b[2]
    
    q = torch.zeros_like(grid)
    q[0,:,:,:,0] = qx
    q[0,:,:,:,1] = qy
    q[0,:,:,:,2] = qz
    
    return tfun.grid_sample(I_mov, q, align_corners=False, **kwargs)
    

def my_generalized_dice(S1, S2, exponent=1):
    """
    Compute Generalized Dice coefficient (Crum et al., IEEE-TMI, 2006) between segmentations
    
    :param S1: First segmentation image
    :param S2: Second segmentation image
    :param exponent: 
        The exponent to which 1/volume is taken when computing the weights of
        individual labels in GDSC computation (see Table 1 in Crum et al.)
    :returns: Generalized Dice coefficient
    """
    d1, d2 = 0, 0 
    for k in torch.unique(torch.hstack((S1.flatten(), S2.flatten()))):
        if k > 0:
            s1 = S1 == k
            s2 = S2 == k
            vol = (torch.sum(s1) + torch.sum(s2)) / 2
            w = 1 / vol ** exponent
            d1 += w * torch.sum(s1 * s2)
            d2 += w * vol

    return d1 / d2
    
    
class RegistrationExperiment:
    """A helper class to simplify loading data for registration experiments"""
    
    def find_file(self, pattern, sid):
        fn_a = os.path.join(self.data_path,pattern % ('atlas',sid))
        fn_t = os.path.join(self.data_path,pattern % ('target',sid))
        return fn_a if os.path.exists(fn_a) else fn_t        
    
    def __init__(self, fixed_id, moving_id, data_path, **kwargs):
        """Initialize the experiment using 4-digit fixed and moving image ids"""
        self.fixed_id = fixed_id
        self.moving_id = moving_id
        self.data_path = data_path
        
        # Load fixed image
        self.I_fix, self.hdr_fix = my_read_pytorch_image_from_nifti(
            self.find_file('images/%s_2mm_%s_3.nii.gz', self.fixed_id), **kwargs)
        
        # Load moving image
        self.I_mov, _ = my_read_pytorch_image_from_nifti(
            self.find_file('images/%s_2mm_%s_3.nii.gz', self.moving_id), **kwargs)
        
        # Load fixed segmenation
        self.S_fix, _ = my_read_pytorch_image_from_nifti(
            self.find_file('images/%sseg_2mm_%s_3.nii.gz', self.fixed_id), **kwargs)
        
        # Load moving segmentation
        self.S_mov, _ = my_read_pytorch_image_from_nifti(
            self.find_file('images/%sseg_2mm_%s_3.nii.gz', self.moving_id), **kwargs)
        
        # Generate grid
        self.grid = my_create_pytorch_grid(self.I_fix.shape, **kwargs)
        
        # Load the affine transform and convert to PyTorch coordinates
        A_np, b_np = my_read_transform(
            os.path.join(data_path,'affine/affine_fx_%s_mv_%s.mat' % (self.fixed_id,self.moving_id)))

        self.A, self.b = my_numpy_affine_to_pytorch_affine(A_np, b_np, self.I_fix.shape[-3:])
        

def my_majority_vote(segs):
    """
    Peform majority voting among multiple segmentation images
    
    :param segs: List/tuple of [1,1,D,W,H] size tensors representing segmentations
    :returns: [1,1,D,W,H] tensor storing the consensus segmentation
    """
    (mval, midx) = torch.mode(torch.cat(segs), dim=0, keepdim=True)
    return mval